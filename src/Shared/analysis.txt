
Summary: The current-state architecture keeps the front-end state consistent by fetching and observing the entire application state. The future state application architecture only fetches and observes occasionally, making the FE state inconsistent most of the time. Usually this is acceptable. When it isn't, data-fetching gets the latest state, but this doesn't solve for "getting current" on entity deletes as deleted entities are not returned when data is fetched.


Background:
In order to achieve a near real-time consistent view of the application state, the current state architecture eagerly fetches all data and observes all state changes all the time. This has an outsize impact on initial load time and exposes the client to many state updates that are not relevant to the on-screen use-case, negatively impacting UI rendering performance and responsiveness. To address these issues, the future-state architecture lazily-loads and observes data needed for the on-screen use case. This results in a far more scalable approach, as the associated data-fetching and subscription costs are amortized on a per use-case basis, instead of fetching and observing the entire application state. 

The future state architecture "lazy" approach increases the risk that the data required for a use case is 1) not correctly loaded and 2) not observed. Risk #1 is largly mitigated by the fact that the relevant use case(s) need to pass basic acceptance testing as features just won't work without the expected data. Risk #2 depends on a few factors:

- Does the data need to be observed (i.e. real-time)? In many more cases will not be observing state changes. Company, region, communication center, user, and lookup data are all examples of entities that will no longer be observed in real-time (for the time being). 
- What type of expectations do we have for real-time observability? If we are observing data in real-time, there are different degrees to which the real-time data can be leveraged. At the most extreme, we will update live forms with real-time data as it changes. In other cases, there may not be a particular requirement or expectation that the particular data for a particular use case or component may be updated in real time, even though it is technically feasible. 
- How confident can we be that the nescessary state changes are observed? This is largley accomplished by simply the data-fetching requirements and pub-sub topology satisfy the requirements for the given use case. 


Problem Description:
There is a gap in #3 for when entities are deleted. Deleted entities are indicated in the back-end application state by the presence of a `DateDeleted` value, and by their absence from the application state in the front-end architecture. Currently, when a state update is observed that results in the presence of a `DateDeleted` value on an entity, the front-end removes it from its application state. This is problematic in the future state where there is limited observability of state updates and because deleted entities are not returned from GraphQL. If a use-case fetches and observes data, then stops observing when a delete occurs, there is no way without a complete refresh of the application, to remove the deleted entity from the client application state.



Considerations:
I) a) Initial and b) ongoing development cost in terms of effort and complexity
II) Functional considerations: 
- Will the front end (or GraphQL API), need to support the concept of deleted entities? Right now there is an implicit assumption that deleted entities do not exist and do not need to be managed at the API/application layer. 
- What level of risk that an approach would make functional defects more likely
III) Non-functional impact - what will the impact be in terms of performance, responsiveness, reliability, and testability?


Solution Alternatives:
A: Publish entity deletes on a global channel
B: Fetch for a log of deleted entities
C: 
D: Return deleted entities during data-fetching

